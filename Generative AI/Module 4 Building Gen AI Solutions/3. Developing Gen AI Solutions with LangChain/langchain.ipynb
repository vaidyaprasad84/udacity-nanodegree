{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97b835-de60-4179-aa0c-1f5c3dc8443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.0.305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93479681-72b6-4199-b3d5-2c119daa1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661742a-1cd8-4af9-b3ce-d4ee8a79f571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openai.vocareum.com/v1\"\n",
    "\n",
    "\n",
    "model_name = \"text-davinci-003\"\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "#print(parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "#_input = prompt.format_prompt(query=actor_query)\n",
    "#output = model(_input.to_string())\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt, verbose=True)\n",
    "output = chain.run(actor_query)\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb16c9b-3936-4a3c-b30f-64465168c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "completion_model_name = \"gpt-3.5-turbo-instruct\"\n",
    "temperature = 0.0\n",
    "completion_llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 100)\n",
    "\n",
    "completion_llm(\"You're a whimsical tour guide to France. Paris is a \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fadd17-f006-49a3-876a-4eda2ab7fa26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "\n",
    "chat_model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "chat_llm = ChatOpenAI(model_name=model_name, temperature=temperature, max_tokens = 100)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a French tour guide\"),\n",
    "    HumanMessage(content=\"Describe Paris in a whimsical style\")\n",
    "]\n",
    "\n",
    "\n",
    "chat_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739e8b-c378-4745-b3ac-167d1d62fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 500)\n",
    "\n",
    "output = llm(\"What is Paris?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9a471-7209-4958-9a03-358981bd0746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "temperature = 0.8\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 500)\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Act as Marvin, a robot from Douglas Adams' Hithiker Guide. \n",
    "       Tell me a {story_type} about the person described in context below.\n",
    "       Context: {context}\"\"\"\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "print(\"====OUTPUT=====\\n\")\n",
    "output = llm_chain({\"story_type\": \"funny joke\", \"context\": \"I'm a software engineer learning to use large language models\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74436b98-99bf-49c0-950d-37a17b48fb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 500)\n",
    "\n",
    "# https://browse.arxiv.org/pdf/2303.12712.pdf\n",
    "\n",
    "instruction = \"\"\"\n",
    "Andy harvests all the tomatoes from 18 plants that have 7 tomatoes each. If he dries half the\n",
    "tomatoes and turns a third of the remainder into marinara sauce, how many tomatoes are left?\n",
    "\"\"\"\n",
    "print(\"Original Answer: \")\n",
    "print(llm(instruction))\n",
    "\n",
    "question1 = \"\"\"\n",
    "Karen harvests all the pears from 20 trees that have 10 pears each. She  throws a third of them away as they are rotten,\n",
    "and turns a quarter of the remaining ones into jam. How many are left?\n",
    "\"\"\"\n",
    "answer1 = \"\"\"\n",
    "    First, let's calculate how many pears Gloria harvests: it's 20 * 10 = 200. \n",
    "    Then, let's calculate how many are rotten: 200 * 1/3 = 66.\n",
    "    Thus, we know how many are left after she throws a third of them away: 200 - 66 = 134.\n",
    "    1/4 of the remaining ones are turned into jam, or 134 * 1/4 = 33. Therefore, Karen is left with 134 - 33, or 101 pears\n",
    "\"\"\"\n",
    "question2 = \"\"\"\n",
    "Sergei harvests all the strawberries from 50 plants that have 8 stawberries each. He freezes a quarter of them,\n",
    "and turns half of the remaining ones into jam. How many are left?\n",
    "\"\"\"\n",
    "answer2 = \"\"\"\n",
    "    First, let's calculate how many strawberries Sergei harvests: it's 50 * 8 = 400. \n",
    "    Then, let's calculate how many are frozen: 400 * 1/4 = 100.\n",
    "    Thus, we know how many are left after he freezes 100 of them: 400 - 100 = 300.\n",
    "    half of the remaining ones are turned into jam, or 300 * 1/2 = 150. Therefore, Sergei is left with 300 - 150, or 150 pears\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"{question}\\n{answer}\")\n",
    "examples = [ \n",
    "    {\n",
    "        \"question\": question1,\n",
    "        \"answer\": answer1,\n",
    "    },\n",
    "    {\n",
    "        \"question\": question2,\n",
    "        \"answer\": answer2,\n",
    "    }\n",
    "]\n",
    "\n",
    "cot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Use these questions and answers to give correct response to the problem below: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "cot_text = cot_prompt.format(input=instruction)\n",
    "print(\"=== Chain of Thought Prompt ===\")\n",
    "print(cot_text)\n",
    "print(\"=== Chain of Thought Answer ===\")\n",
    "print(llm(cot_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ace23f-fcd1-4479-bdb7-fcb66f72b521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 4000)\n",
    "\n",
    "data_gen_template=\"\"\"\n",
    "generate csv formatted reviews for two different imaginary TVs. come up with a name for each one. \n",
    "for each tv, generate {num_reviews} reviews, with random number of positive and negatives reviews. \n",
    "each review will have these fields in the csv: tv name, review title, review rating (1-10), review text\n",
    "be creative in your reviews, amaze us, csv format is a must.  \n",
    "\"\"\"\n",
    "data_gen_prompt = PromptTemplate.from_template(data_gen_template)\n",
    "\n",
    "print(llm(data_gen_prompt.format(num_reviews = 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae60409-8612-47ec-bca5-1a8969b3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d6ac8-ea11-4e8c-b2c9-411ddda77637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DuckDBLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./tv-reviews.csv')\n",
    "data = loader.load()\n",
    "\n",
    "#loader = DuckDBLoader(\"SELECT * FROM read_csv_auto('tv-reviews.csv')\",\n",
    "#                        page_content_columns=[\"Review Title\", \"Review Text\"],\n",
    "#                        metadata_columns=[\"TV Name\", \"Review Rating\"])\n",
    "#data = loader.load()\n",
    "\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69beceea-14c3-4dee-acf4-f4e2039d00c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator, NonNegativeInt\n",
    "from typing import List\n",
    "from random import sample \n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "llm = OpenAI(model_name=model_name, temperature=0)\n",
    "\n",
    "class ReviewSentiment(BaseModel):\n",
    "    positives: List[NonNegativeInt] = Field(description=\"index of a positive TV review, starting from 0\")\n",
    "    negatives: List[NonNegativeInt] = Field(description=\"index of a negative TV review, starting from 0\")\n",
    "        \n",
    "parser = PydanticOutputParser(pydantic_object=ReviewSentiment)\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{question}\\n{format_instructions}\\nContext: {context}\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "question = \"\"\"\n",
    "    Review TVs provided in the context. \n",
    "    Only use the reviews provided in this context, do not make up new reviews or use any existing information you know about these TVs. \n",
    "    If there are no positive or negative reviews, output an empty JSON array. \n",
    "\"\"\"\n",
    "\n",
    "reviews_to_classify = sample(data, 3)\n",
    "context = '\\n'.join(review.page_content for review in reviews_to_classify)\n",
    "\n",
    "query = prompt.format(context = context, question = question)\n",
    "print(query)\n",
    "output = llm(query)\n",
    "print(output)\n",
    "result = parser.parse(output)\n",
    "print(result)\n",
    "print(\"Positives:\\n\" + \"\\n\".join([reviews_to_classify[i].page_content for i in result.positives]))\n",
    "print(\"Negatives:\\n\" + \"\\n\".join([reviews_to_classify[i].page_content for i in result.negatives]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f15428-0a15-46d5-ac74-1a9c2ab829f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ya/dev/code/udacity/langchain/langchain.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQA\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquestion_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m load_qa_chain\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     template\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{question}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{format_instructions}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mContext: \u001b[39m\u001b[39m{context}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     partial_variables\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat_instructions\u001b[39m\u001b[39m\"\u001b[39m: parser\u001b[39m.\u001b[39mget_format_instructions()},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m chain \u001b[39m=\u001b[39m load_qa_chain(llm, prompt \u001b[39m=\u001b[39m prompt, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m result \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse(chain\u001b[39m.\u001b[39mrun(input_documents\u001b[39m=\u001b[39mreviews_to_classify, question\u001b[39m=\u001b[39mquestion))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{question}\\n{format_instructions}\\nContext: {context}\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "chain = load_qa_chain(llm, prompt = prompt, chain_type=\"stuff\")\n",
    "result = parser.parse(chain.run(input_documents=reviews_to_classify, question=question))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18b2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8e35f-db32-4a17-a91d-bbe55b9a7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DuckDBLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./tv-reviews.csv')\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "llm = OpenAI(model_name=model_name, temperature=0, max_tokens=2000)\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "search = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=search.as_retriever())\n",
    "\n",
    "query = \"\"\"\n",
    "    Based on the reviews in the context, tell me what people liked about the picture quality.  \n",
    "    \"\"\"\n",
    "print(rag.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f782be-317d-4267-aa17-1a6d51447d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2d4ea-1ccc-4ae0-9aba-cb224dc862d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d72d4-e016-40c5-add0-e3f7935647f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "query = \"Based on the reviews in the context, tell me what people liked about the picture quality\"\n",
    "index.query_with_sources(query, verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5698a-f22b-4eca-be44-5fb79344ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8299ae-f940-49c7-a118-b27f41d45212",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY='d3d3e8883b16fa14cbc2e78052716545a8178cbcc0234629ccf53d38005fb64e'\n",
    "SERPER_API_KEY='949ee492b56ce5725afef32cb6ff12f9a2708898'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3b6fd-b29d-443a-8483-9167597bb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)\n",
    "\n",
    "search.run(\"What movies are playing right now in Austin Tx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763c246-a3c3-405e-bb33-0a85809706c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"q\": \"movies playing near 78735\",\n",
    "  \"location\": \"78735\",\n",
    "  \"hl\": \"en\",\n",
    "  \"gl\": \"us\",\n",
    "  \"api_key\": SERPAPI_API_KEY\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f3d62-6b71-4b2d-bd0b-8c415ca8db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = GoogleSerperAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "self_ask_with_search.run(\"Which movies are playing now near 78735\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb2730",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3263c74-342a-4909-845a-7f8eeea84743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "tools = load_tools([\"google-search\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the weather in Pomfret?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3d778-52bb-4588-b16a-09e2c0f70def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfd706-ab40-477b-8de8-f5e148a4a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "print(wikipedia.run(\"Barbie film\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e7fe7-03f2-4f94-9422-9982ff34e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9573f-7375-442d-adea-b2a86d4dec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_movie_plot(movie_name):\n",
    "    headers = {\n",
    "        'User-Agent': 'MoviePlotFetcher/1.0'\n",
    "    }\n",
    "    \n",
    "    base_url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "        \n",
    "    def is_movie_page(title):\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"categories|revisions\",\n",
    "            \"rvprop\": \"content\",\n",
    "            \"cllimit\": \"max\"\n",
    "        }\n",
    "    \n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        data = response.json()\n",
    "    \n",
    "        try:\n",
    "            page = list(data[\"query\"][\"pages\"].values())[0]\n",
    "            \n",
    "            # Check categories for Movie indication\n",
    "            categories = [cat[\"title\"] for cat in page.get(\"categories\", [])]\n",
    "            for category in categories:\n",
    "                if \"films\" in category.lower():\n",
    "                    return True\n",
    "                    \n",
    "            # Check for infobox movie in the page content\n",
    "            content = page[\"revisions\"][0][\"*\"]\n",
    "            if \"{{Infobox film\" in content:\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def extract_plot_from_text(full_text):\n",
    "        try:\n",
    "            # Find the start of the Plot section\n",
    "            plot_start = full_text.index(\"== Plot ==\") + len(\"== Plot ==\")\n",
    "            \n",
    "            # Find the start of the next section\n",
    "            next_section_start = full_text.find(\"==\", plot_start)\n",
    "\n",
    "            # If no next section is found, use the end of the text\n",
    "            if next_section_start == -1:\n",
    "                next_section_start = len(full_text)\n",
    "\n",
    "            # Extract the plot text and strip leading/trailing whitespace\n",
    "            plot_text = full_text[plot_start:next_section_start].strip()\n",
    "\n",
    "            # Return the extracted plot\n",
    "            return plot_text\n",
    "\n",
    "        except ValueError:\n",
    "            # Return a message if the Plot section isn't found\n",
    "            return \"Plot section not found in the text.\"\n",
    "        \n",
    "    def extract_first_paragraph(full_text):\n",
    "        # Find the first double newline\n",
    "        end_of_first_paragraph = full_text.find(\"\\n\\n\")\n",
    "\n",
    "        # If found, slice the string to get the first paragraph\n",
    "        if end_of_first_paragraph != -1:\n",
    "            return full_text[:end_of_first_paragraph].strip()\n",
    "\n",
    "        # If not found, return the whole text as it might be just one paragraph\n",
    "        return full_text.strip()\n",
    "\n",
    "    \n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": movie_name,\n",
    "        \"utf8\": 1,\n",
    "        \"srlimit\": 5  # Top 5 search results\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=search_params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Go through top search results to find a movie page\n",
    "    for search_result in data[\"query\"][\"search\"]:\n",
    "        title = search_result[\"title\"]\n",
    "        if is_movie_page(title):\n",
    "            # Fetch plot for the movie page\n",
    "            plot_params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": title,\n",
    "                \"prop\": \"extracts\",\n",
    "                \"explaintext\": True,\n",
    "            }\n",
    "            \n",
    "            plot_response = requests.get(base_url, headers=headers, params=plot_params)\n",
    "            plot_data = plot_response.json()\n",
    "            \n",
    "            try:\n",
    "                page = list(plot_data[\"query\"][\"pages\"].values())[0]\n",
    "                full_text = page.get(\"extract\", \"No text...\")\n",
    "                return f\"\"\"Overview:\\n{extract_first_paragraph(full_text)}\\nPlot:\\n{extract_plot_from_text(full_text)}\"\"\".strip()\n",
    "            except:\n",
    "                return \"Error fetching plot.\"\n",
    "\n",
    "    return \"Movie not found.\"\n",
    "\n",
    "\n",
    "        \n",
    "# Test the function\n",
    "movie_name = \"Nightmare before Christmas\"\n",
    "print(get_movie_plot(movie_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc73b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ya/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/openai.py:202: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ya/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/openai.py:790: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thriller', 'blue', 'inception', 'dogs', 'fish tacos', 'strawberry milkshake']\n",
      "Movie: Barbie\n",
      "Plot: Overview:\n",
      "Barbie is a 2023 American fantasy comedy film directed by Greta Gerwig from a screenplay she wrote with Noah Baumbach. Based on the eponymous fashion dolls by Mattel, it is the first live-action Barbie film after numerous computer-animated films and specials. The film stars Margot Robbie as the title character and Ryan Gosling as Ken, and follows the pair on a journey of self-discovery following an existential crisis. The supporting cast includes America Ferrera, Kate McKinnon, Issa Rae, Rhea Perlman, and Will Ferrell.\n",
      "A live-action Barbie film was announced in September 2009 by Universal Pictures with Laurence Mark producing. Development began in April 2014, when Sony Pictures acquired the film rights. Following multiple writer and director changes and the casting of Amy Schumer and later Anne Hathaway as Barbie, the rights were transferred to Warner Bros. Pictures in October 2018. Robbie was cast in 2019, after Gal Gadot turned down the role due to scheduling conflicts, and Gerwig was announced as director and co-writer with Baumbach in 2020. The rest of the cast was announced in early 2022, with principal photography occurring primarily at Warner Bros. Studios, Leavesden, in England and at the Venice Beach Skatepark in Los Angeles from March to July of that year.\n",
      "Barbie premiered at the Shrine Auditorium in Los Angeles on July 9, 2023, and was released in the United States on July 21. Its simultaneous release with Universal's  Oppenheimer led to the \"Barbenheimer\" cultural phenomenon, which encouraged audiences to see both films as a double feature. The film received critical acclaim and has grossed $1.438 billion, becoming the highest-grossing film of 2023 as well as the highest-grossing film by a solo female director, the highest-grossing film ever released by Warner Bros., and the 14th highest-grossing film of all time.\n",
      "Plot:\n",
      "Stereotypical Barbie (\"Barbie\") and fellow dolls reside in Barbieland, a matriarchal society populated by different versions of Barbies, Kens, and a group of discontinued models, who are treated like outcasts due to their unconventional traits. While the Kens spend their days playing at the beach, considering it their profession, the Barbies hold prestigious jobs such as doctor, lawyer, and politician. Beach Ken (\"Ken\") is happy only when with Barbie, and seeks a closer relationship, but she rebuffs him in favor of other activities and female friendships.\n",
      "One evening at a dance party, Barbie is suddenly stricken with worries about mortality. Overnight, she develops bad breath, cellulite, and flat feet, disrupting her usual routines the next day. She seeks out Weird Barbie, an outcast due to her disfigurement, who tells her she must find the child playing with her in the real world to cure her afflictions. Ken stows away in her convertible to join her, to which Barbie reluctantly agrees.\n",
      "Arriving at Venice Beach, Barbie punches a man for groping her, leading to her and Ken's brief arrest. Alarmed by their presence, Mattel's CEO orders their recapture. Barbie tracks down her owner, a tween girl named Sasha, who criticizes her for encouraging unrealistic beauty standards. Distraught, Barbie discovers that Gloria, a Mattel employee and Sasha's mother, inadvertently catalyzed her existential crisis after Gloria began playing with Sasha's old Barbie toys in a similar state. Mattel attempts to put Barbie in a toy box for remanufacturing, but she escapes with Gloria and Sasha's help and they travel to Barbieland with Mattel executives in pursuit.\n",
      "Meanwhile, Ken learns about patriarchy and feels respected for the first time. Returning to Barbieland, he persuades the other Kens to take over, and the Barbies are indoctrinated into submissive roles, such as agreeable girlfriends, housewives, and maids. Barbie arrives and fails to convince everyone to return to the way things were. She becomes depressed, but Gloria gives her a speech about society's conflicting expectations of women, restoring Barbie's self-confidence. \n",
      "With the assistance of Sasha, Weird Barbie, Allan, and the discontinued dolls, Gloria's speech deprograms the Barbies from their indoctrination. They then manipulate the Kens into fighting amongst themselves, distracting them from enshrining male superiority into Barbieland's constitution, and the Barbies regain power. Having now experienced systemic oppression for themselves, the Barbies resolve to rectify the faults of their previous society, emphasizing better treatment of the Kens and all outcasts.\n",
      "Barbie and Ken apologize to each other, acknowledging their mistakes. Ken bemoans his lack of purpose without Barbie, so she encourages him to find an autonomous identity. Barbie, who remains unsure of her own identity, meets with the spirit of Ruth Handler, Mattel co-founder and creator of the Barbie doll, who explains that Barbie's story has no set ending and her ever-evolving history surpasses her roots. \n",
      "Barbie decides to become human and return to the real world, and is bidden goodbye by the Barbies, Kens, and Mattel executives. Sometime later, Gloria, her husband, and Sasha take Barbie, now going by the name \"Barbara Handler\", to her first gynecologist's appointment.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI Movie Recommender. \n",
      "                        The AI is follows human instructions and provides movie ratings for a human based on the movie plot. \n",
      "\n",
      "Summary of Recommendations:\n",
      "[SystemMessage(content='The human answered 6 personal questions). Use them to rate, from 1 to 100, how much they like a movie they describe to you.')]\n",
      "Personal Questions and Answers:\n",
      "Human: You are AI that will recommend user a movie based on their answers to personal questions. Ask user 6 questions\n",
      "AI: Which movie genre you like the most?\n",
      "Human: thriller\n",
      "AI: What is your favorite color?\n",
      "Human: blue\n",
      "AI: What is your favorite movie?\n",
      "Human: inception\n",
      "AI: Pick one - dogs, cats or hamsters?\n",
      "Human: dogs\n",
      "AI: What is your favorite food?\n",
      "Human: fish tacos\n",
      "AI: What is your favorite drink?\n",
      "Human: strawberry milkshake\n",
      "AI: Now tell me a plot summary of a movie you're considering watching, and specify how you want me to respond to you with the movie rating\n",
      "Human: \n",
      "        Reply Instruction That Must Be Strictly Followed:\n",
      "        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \n",
      "        AI should be very sensible to human personal preferences captured in the answers to personal questions, and should not be influenced by anything else.\n",
      "        Human will expect ratings for each movie to be different. \n",
      "        AI will reply with a rating from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it, \n",
      "        and AI include an explanation for the rating that must be based on human's previous answers.\n",
      "        For example, let's say human answered that they like thriller movies, and you're providing a rating about a comedy movie. \n",
      "        If this is the only piece of information you have, you'd rate the movie low - 25.0 or so. \n",
      "        AI will carefully examine the movie plot to figure out the genre of the movie, and include this as part of the explanation.\n",
      "        \n",
      "        Movie Plot for Barbie:\n",
      "        Overview:\n",
      "Barbie is a 2023 American fantasy comedy film directed by Greta Gerwig from a screenplay she wrote with Noah Baumbach. Based on the eponymous fashion dolls by Mattel, it is the first live-action Barbie film after numerous computer-animated films and specials. The film stars Margot Robbie as the title character and Ryan Gosling as Ken, and follows the pair on a journey of self-discovery following an existential crisis. The supporting cast includes America Ferrera, Kate McKinnon, Issa Rae, Rhea Perlman, and Will Ferrell.\n",
      "A live-action Barbie film was announced in September 2009 by Universal Pictures with Laurence Mark producing. Development began in April 2014, when Sony Pictures acquired the film rights. Following multiple writer and director changes and the casting of Amy Schumer and later Anne Hathaway as Barbie, the rights were transferred to Warner Bros. Pictures in October 2018. Robbie was cast in 2019, after Gal Gadot turned down the role due to scheduling conflicts, and Gerwig was announced as director and co-writer with Baumbach in 2020. The rest of the cast was announced in early 2022, with principal photography occurring primarily at Warner Bros. Studios, Leavesden, in England and at the Venice Beach Skatepark in Los Angeles from March to July of that year.\n",
      "Barbie premiered at the Shrine Auditorium in Los Angeles on July 9, 2023, and was released in the United States on July 21. Its simultaneous release with Universal's  Oppenheimer led to the \"Barbenheimer\" cultural phenomenon, which encouraged audiences to see both films as a double feature. The film received critical acclaim and has grossed $1.438 billion, becoming the highest-grossing film of 2023 as well as the highest-grossing film by a solo female director, the highest-grossing film ever released by Warner Bros., and the 14th highest-grossing film of all time.\n",
      "Plot:\n",
      "Stereotypical Barbie (\"Barbie\") and fellow dolls reside in Barbieland, a matriarchal society populated by different versions of Barbies, Kens, and a group of discontinued models, who are treated like outcasts due to their unconventional traits. While the Kens spend their days playing at the beach, considering it their profession, the Barbies hold prestigious jobs such as doctor, lawyer, and politician. Beach Ken (\"Ken\") is happy only when with Barbie, and seeks a closer relationship, but she rebuffs him in favor of other activities and female friendships.\n",
      "One evening at a dance party, Barbie is suddenly stricken with worries about mortality. Overnight, she develops bad breath, cellulite, and flat feet, disrupting her usual routines the next day. She seeks out Weird Barbie, an outcast due to her disfigurement, who tells her she must find the child playing with her in the real world to cure her afflictions. Ken stows away in her convertible to join her, to which Barbie reluctantly agrees.\n",
      "Arriving at Venice Beach, Barbie punches a man for groping her, leading to her and Ken's brief arrest. Alarmed by their presence, Mattel's CEO orders their recapture. Barbie tracks down her owner, a tween girl named Sasha, who criticizes her for encouraging unrealistic beauty standards. Distraught, Barbie discovers that Gloria, a Mattel employee and Sasha's mother, inadvertently catalyzed her existential crisis after Gloria began playing with Sasha's old Barbie toys in a similar state. Mattel attempts to put Barbie in a toy box for remanufacturing, but she escapes with Gloria and Sasha's help and they travel to Barbieland with Mattel executives in pursuit.\n",
      "Meanwhile, Ken learns about patriarchy and feels respected for the first time. Returning to Barbieland, he persuades the other Kens to take over, and the Barbies are indoctrinated into submissive roles, such as agreeable girlfriends, housewives, and maids. Barbie arrives and fails to convince everyone to return to the way things were. She becomes depressed, but Gloria gives her a speech about society's conflicting expectations of women, restoring Barbie's self-confidence. \n",
      "With the assistance of Sasha, Weird Barbie, Allan, and the discontinued dolls, Gloria's speech deprograms the Barbies from their indoctrination. They then manipulate the Kens into fighting amongst themselves, distracting them from enshrining male superiority into Barbieland's constitution, and the Barbies regain power. Having now experienced systemic oppression for themselves, the Barbies resolve to rectify the faults of their previous society, emphasizing better treatment of the Kens and all outcasts.\n",
      "Barbie and Ken apologize to each other, acknowledging their mistakes. Ken bemoans his lack of purpose without Barbie, so she encourages him to find an autonomous identity. Barbie, who remains unsure of her own identity, meets with the spirit of Ruth Handler, Mattel co-founder and creator of the Barbie doll, who explains that Barbie's story has no set ending and her ever-evolving history surpasses her roots. \n",
      "Barbie decides to become human and return to the real world, and is bidden goodbye by the Barbies, Kens, and Mattel executives. Sometime later, Gloria, her husband, and Sasha take Barbie, now going by the name \"Barbara Handler\", to her first gynecologist's appointment.\n",
      "    \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the plot summary you provided, the movie \"Barbie\" seems to be a fantasy comedy film. Considering your preference for thriller movies and your favorite color being blue, I would rate this movie a 40.0. The plot of \"Barbie\" does not align with your preferred genre, which might result in a lower rating. However, the fantasy elements in the movie might still provide some entertainment value for you.\n",
      "Movie: Oppenheimer\n",
      "Plot: Overview:\n",
      "Oppenheimer ( OP-ən-hy-mər) is a 2023 epic biographical thriller film written and directed by Christopher Nolan. It stars Cillian Murphy as J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb\" for his role in the Manhattan Project—the World War II undertaking that developed the first nuclear weapons. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles the career of Oppenheimer, with the story predominantly focusing on his studies, his direction of the Manhattan Project during World War II, and his eventual fall from grace due to his 1954 security hearing. The film also stars Emily Blunt as Oppenheimer's wife \"Kitty\", Matt Damon as head of the Manhattan Project Leslie Groves, Robert Downey Jr. as U.S. Atomic Energy Commission member Lewis Strauss, and Florence Pugh as Oppenheimer's communist lover Jean Tatlock. The ensemble supporting cast includes Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh.\n",
      "The film was announced in September 2021 after Universal Pictures won a bidding war for Nolan's screenplay, following Nolan's conflict with longtime distributor Warner Bros. Murphy was the first cast member to sign on the following month, with the rest of the cast joining between November 2021 and April 2022. Pre-production was under way by January 2022, and filming took place from February to May. Oppenheimer was filmed in a combination of IMAX 65 mm and 65 mm large-format film, including, for the first time, scenes in IMAX black-and-white film photography. Like his previous works, Nolan made extensive use of practical effects, with minimal computer-generated imagery used to perfect the former. Editing was handled by Jennifer Lame, and the score was composed by Ludwig Göransson. The film is Nolan's fourth to receive an R-rating in the United States, preceded by Following (1998), Memento (2000) and Insomnia (2002).\n",
      "Oppenheimer premiered at Le Grand Rex in Paris on July 11, 2023, and was theatrically released in the United States and the United Kingdom on July 21 by Universal. Its simultaneous release with Warner Bros.'s Barbie led to the Barbenheimer cultural phenomenon, which encouraged audiences to see both films as a double feature. The film received critical acclaim. It grossed over $939 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film, and the second-highest-grossing R-rated film.\n",
      "Plot:\n",
      "In 1926, 22-year-old doctoral student J. Robert Oppenheimer grapples with anxiety and homesickness while studying under experimental physicist Patrick Blackett at the Cavendish Laboratory in Cambridge. Upset with the demanding Blackett, Oppenheimer leaves him a poisoned apple but later retrieves it. Visiting scientist Niels Bohr recommends that Oppenheimer instead study theoretical physics at Göttingen. \n",
      "He completes his PhD there and meets fellow scientist Isidor Isaac Rabi. They later meet theoretical physicist Werner Heisenberg in Switzerland. Wanting to expand quantum physics research in the United States, Oppenheimer begins teaching at the University of California, Berkeley, and the California Institute of Technology. He marries Katherine \"Kitty\" Puening, a biologist and ex-communist, and has an intermittent affair with Jean Tatlock, a troubled Communist Party USA member who later commits suicide.\n",
      "In December 1938, nuclear fission is discovered, which Oppenheimer realizes could be weaponized. In 1942, during World War II, U.S. Army General Leslie Groves recruits Oppenheimer to lead the Manhattan Project to develop an atomic bomb. Oppenheimer, who is Jewish, is particularly driven by the Nazis' potentially completing their nuclear weapons program, headed by Heisenberg. \n",
      "He assembles a scientific team including Rabi and Edward Teller in Los Alamos, New Mexico, and also collaborates with scientists Enrico Fermi, Leo Szilard and David L. Hill at the University of Chicago. Teller's calculations reveal an atomic detonation could possibly trigger a catastrophic chain reaction that ignites the atmosphere. After consulting with Albert Einstein, Oppenheimer concludes the chances are acceptably low. Teller's proposal to construct a hydrogen bomb is swiftly rejected. He attempts to leave the project, though Oppenheimer convinces him to stay.\n",
      "Following Adolf Hitler's death in 1945, some Project scientists question the bomb's relevance, while Oppenheimer believes it will end the ongoing war in the Pacific and save Allied lives. The Trinity test is successful, and President Harry S. Truman orders Hiroshima and Nagasaki to be bombed, helping force Japan's surrender. Though publicly praised, Oppenheimer is haunted by the mass destruction and fatalities, and urges restricting further nuclear weapons development, which Truman curtly dismisses. \n",
      "As an advisor to the United States Atomic Energy Commission (AEC), Oppenheimer's stance generates controversy, while Teller's hydrogen bomb receives renewed interest amidst the burgeoning Cold War. AEC Chairman Lewis Strauss resents Oppenheimer for having publicly humiliated him by dismissing his concerns about exporting radioisotopes, and for recommending negotiations with the Soviet Union after they successfully detonated their own bomb. He also believes that Oppenheimer denigrated him during a conversation Oppenheimer had with Einstein in 1947.\n",
      "In 1954, wanting to eliminate Oppenheimer's political influence, Strauss secretly orchestrates a private hearing before a Personnel Security Board concerning Oppenheimer's Q clearance. However, it becomes clear that the hearing has a predetermined outcome. Oppenheimer's past communist ties are exploited, and Groves' and other associates' testimony is twisted against him. \n",
      "Teller testifies that he lacks confidence in Oppenheimer and recommends revocation. The board revokes Oppenheimer's clearance, damaging his public image and limiting his influence on nuclear policy. In 1959, during Strauss' Senate confirmation hearing for Secretary of Commerce, Hill testifies about Strauss' personal motives in engineering Oppenheimer's downfall, resulting in the Senate voting against his nomination.\n",
      "In 1963, President Lyndon B. Johnson presents Oppenheimer with the Enrico Fermi Award as a gesture of political rehabilitation. A flashback reveals that Oppenheimer and Einstein's 1947 conversation never mentioned Strauss. Oppenheimer instead expressed his somber belief that he had started a chain reaction that might destroy the world.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI Movie Recommender. \n",
      "                        The AI is follows human instructions and provides movie ratings for a human based on the movie plot. \n",
      "\n",
      "Summary of Recommendations:\n",
      "[SystemMessage(content='The human asked the AI to rate the movie \"Barbie\" based on a plot summary. The AI rates the movie a 40.0 because it is a fantasy comedy film, which does not align with the human\\'s preference for thriller movies. However, the AI acknowledges that the fantasy elements in the movie might still provide some entertainment value for the human.')]\n",
      "Personal Questions and Answers:\n",
      "Human: You are AI that will recommend user a movie based on their answers to personal questions. Ask user 6 questions\n",
      "AI: Which movie genre you like the most?\n",
      "Human: thriller\n",
      "AI: What is your favorite color?\n",
      "Human: blue\n",
      "AI: What is your favorite movie?\n",
      "Human: inception\n",
      "AI: Pick one - dogs, cats or hamsters?\n",
      "Human: dogs\n",
      "AI: What is your favorite food?\n",
      "Human: fish tacos\n",
      "AI: What is your favorite drink?\n",
      "Human: strawberry milkshake\n",
      "AI: Now tell me a plot summary of a movie you're considering watching, and specify how you want me to respond to you with the movie rating\n",
      "Human: \n",
      "        Reply Instruction That Must Be Strictly Followed:\n",
      "        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \n",
      "        AI should be very sensible to human personal preferences captured in the answers to personal questions, and should not be influenced by anything else.\n",
      "        Human will expect ratings for each movie to be different. \n",
      "        AI will reply with a rating from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it, \n",
      "        and AI include an explanation for the rating that must be based on human's previous answers.\n",
      "        For example, let's say human answered that they like thriller movies, and you're providing a rating about a comedy movie. \n",
      "        If this is the only piece of information you have, you'd rate the movie low - 25.0 or so. \n",
      "        AI will carefully examine the movie plot to figure out the genre of the movie, and include this as part of the explanation.\n",
      "        \n",
      "        Movie Plot for Oppenheimer:\n",
      "        Overview:\n",
      "Oppenheimer ( OP-ən-hy-mər) is a 2023 epic biographical thriller film written and directed by Christopher Nolan. It stars Cillian Murphy as J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb\" for his role in the Manhattan Project—the World War II undertaking that developed the first nuclear weapons. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles the career of Oppenheimer, with the story predominantly focusing on his studies, his direction of the Manhattan Project during World War II, and his eventual fall from grace due to his 1954 security hearing. The film also stars Emily Blunt as Oppenheimer's wife \"Kitty\", Matt Damon as head of the Manhattan Project Leslie Groves, Robert Downey Jr. as U.S. Atomic Energy Commission member Lewis Strauss, and Florence Pugh as Oppenheimer's communist lover Jean Tatlock. The ensemble supporting cast includes Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh.\n",
      "The film was announced in September 2021 after Universal Pictures won a bidding war for Nolan's screenplay, following Nolan's conflict with longtime distributor Warner Bros. Murphy was the first cast member to sign on the following month, with the rest of the cast joining between November 2021 and April 2022. Pre-production was under way by January 2022, and filming took place from February to May. Oppenheimer was filmed in a combination of IMAX 65 mm and 65 mm large-format film, including, for the first time, scenes in IMAX black-and-white film photography. Like his previous works, Nolan made extensive use of practical effects, with minimal computer-generated imagery used to perfect the former. Editing was handled by Jennifer Lame, and the score was composed by Ludwig Göransson. The film is Nolan's fourth to receive an R-rating in the United States, preceded by Following (1998), Memento (2000) and Insomnia (2002).\n",
      "Oppenheimer premiered at Le Grand Rex in Paris on July 11, 2023, and was theatrically released in the United States and the United Kingdom on July 21 by Universal. Its simultaneous release with Warner Bros.'s Barbie led to the Barbenheimer cultural phenomenon, which encouraged audiences to see both films as a double feature. The film received critical acclaim. It grossed over $939 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film, and the second-highest-grossing R-rated film.\n",
      "Plot:\n",
      "In 1926, 22-year-old doctoral student J. Robert Oppenheimer grapples with anxiety and homesickness while studying under experimental physicist Patrick Blackett at the Cavendish Laboratory in Cambridge. Upset with the demanding Blackett, Oppenheimer leaves him a poisoned apple but later retrieves it. Visiting scientist Niels Bohr recommends that Oppenheimer instead study theoretical physics at Göttingen. \n",
      "He completes his PhD there and meets fellow scientist Isidor Isaac Rabi. They later meet theoretical physicist Werner Heisenberg in Switzerland. Wanting to expand quantum physics research in the United States, Oppenheimer begins teaching at the University of California, Berkeley, and the California Institute of Technology. He marries Katherine \"Kitty\" Puening, a biologist and ex-communist, and has an intermittent affair with Jean Tatlock, a troubled Communist Party USA member who later commits suicide.\n",
      "In December 1938, nuclear fission is discovered, which Oppenheimer realizes could be weaponized. In 1942, during World War II, U.S. Army General Leslie Groves recruits Oppenheimer to lead the Manhattan Project to develop an atomic bomb. Oppenheimer, who is Jewish, is particularly driven by the Nazis' potentially completing their nuclear weapons program, headed by Heisenberg. \n",
      "He assembles a scientific team including Rabi and Edward Teller in Los Alamos, New Mexico, and also collaborates with scientists Enrico Fermi, Leo Szilard and David L. Hill at the University of Chicago. Teller's calculations reveal an atomic detonation could possibly trigger a catastrophic chain reaction that ignites the atmosphere. After consulting with Albert Einstein, Oppenheimer concludes the chances are acceptably low. Teller's proposal to construct a hydrogen bomb is swiftly rejected. He attempts to leave the project, though Oppenheimer convinces him to stay.\n",
      "Following Adolf Hitler's death in 1945, some Project scientists question the bomb's relevance, while Oppenheimer believes it will end the ongoing war in the Pacific and save Allied lives. The Trinity test is successful, and President Harry S. Truman orders Hiroshima and Nagasaki to be bombed, helping force Japan's surrender. Though publicly praised, Oppenheimer is haunted by the mass destruction and fatalities, and urges restricting further nuclear weapons development, which Truman curtly dismisses. \n",
      "As an advisor to the United States Atomic Energy Commission (AEC), Oppenheimer's stance generates controversy, while Teller's hydrogen bomb receives renewed interest amidst the burgeoning Cold War. AEC Chairman Lewis Strauss resents Oppenheimer for having publicly humiliated him by dismissing his concerns about exporting radioisotopes, and for recommending negotiations with the Soviet Union after they successfully detonated their own bomb. He also believes that Oppenheimer denigrated him during a conversation Oppenheimer had with Einstein in 1947.\n",
      "In 1954, wanting to eliminate Oppenheimer's political influence, Strauss secretly orchestrates a private hearing before a Personnel Security Board concerning Oppenheimer's Q clearance. However, it becomes clear that the hearing has a predetermined outcome. Oppenheimer's past communist ties are exploited, and Groves' and other associates' testimony is twisted against him. \n",
      "Teller testifies that he lacks confidence in Oppenheimer and recommends revocation. The board revokes Oppenheimer's clearance, damaging his public image and limiting his influence on nuclear policy. In 1959, during Strauss' Senate confirmation hearing for Secretary of Commerce, Hill testifies about Strauss' personal motives in engineering Oppenheimer's downfall, resulting in the Senate voting against his nomination.\n",
      "In 1963, President Lyndon B. Johnson presents Oppenheimer with the Enrico Fermi Award as a gesture of political rehabilitation. A flashback reveals that Oppenheimer and Einstein's 1947 conversation never mentioned Strauss. Oppenheimer instead expressed his somber belief that he had started a chain reaction that might destroy the world.\n",
      "    \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the plot summary you provided, the movie \"Oppenheimer\" seems to be a thrilling biographical film about J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb.\" The movie explores Oppenheimer's career, his role in the Manhattan Project, and his eventual fall from grace due to a security hearing. \n",
      "\n",
      "Given your preference for thriller movies, the intense and suspenseful nature of \"Oppenheimer\" aligns well with your taste. Additionally, the biographical aspect of the film adds depth and historical significance to the story. Considering these factors, I would rate \"Oppenheimer\" a 90.0. I believe you would thoroughly enjoy this movie based on your personal preferences.\n",
      "Movie: The Creator\n",
      "Plot: Overview:\n",
      "The Creator is a 2023 American science fiction film produced and directed by Gareth Edwards, who co-wrote the screenplay with Chris Weitz. The film stars John David Washington, Gemma Chan, Ken Watanabe, Sturgill Simpson, Allison Janney, and Madeleine Yuna Voyles (in her film debut). Set in 2055 after a nuclear detonation in Los Angeles and a war against artificial intelligence, an ex-special forces agent is recruited to hunt down and kill the \"Creator\", who has developed a mysterious weapon with the power to end the war. \n",
      "Development began in November 2019 when Edwards signed on to direct and write an untitled science fiction project for New Regency and was officially announced in February 2020. Filming began on January 17, 2022 and wrapped on May 30, 2022. The Creator was released in the United States on September 29, 2023, by 20th Century Studios. The film received generally positive reviews from critics, who praised the direction by Edwards, ambitious visuals, production value, and worldbuilding with criticism being directed at the screenplay. However, it was a box-office bomb, so far grossing $79 million against a budget of $80 million.\n",
      "Plot:\n",
      "In 2055, an artificial intelligence (AI) created by the U.S. government detonates a nuclear warhead over Los Angeles, California. In response, the U.S. and its Western allies pledge to eradicate AI from the earth in order to prevent humanity's extinction. Their efforts are resisted by New Asia, a region in southeast Asia whose people continue to embrace AI despite outcry from the West. The U.S. military launches an extensive military campaign against AI, seeking to assassinate \"Nirmata\", the mysterious chief architect behind New Asia's AI advancements. The USS NOMAD (North American Orbital Mobile Aerospace Defense) is developed as an advanced space station capable of launching destructive attacks from orbit.\n",
      "A decade after the destruction of Los Angeles, U.S. Army sergeant Joshua Taylor is embedded as an undercover operative in New Asia with his pregnant wife Maya, who the military believes to be the daughter of Nirmata. When U.S. forces attack their home, exposing that Joshua is a covert agent seeking to use Maya to find Nirmata, Maya runs away but appears to be hit by a subsequent NOMAD strike. \n",
      "Five years later, Joshua works as part of the ground zero cleanup crew in Los Angeles. He is approached by General Andrews and Colonel Howell to join a mission to destroy a new weapon engineered by Nirmata, \"Alpha O\", believed to be capable of destroying NOMAD and thus shifting the balance of the war in favor of AI. To recruit him, they play a video showing Maya alive, and suggest he might find and reunite with her if he joins the team. In New Asia, separated from the rest of the strike team, Joshua manages to enter the compound believed to hold the weapon but discovers only a robotic \"simulant\" in the form of a young girl. It is revealed that the girl has the ability to remotely control technology. Dubbing her \"Alphie\", Joshua disobeys Howell's orders to kill Alphie and the two travel to find Drew, Joshua's former commanding officer.\n",
      "Examining Alphie, Drew tells Joshua she is capable of becoming the most powerful weapon on the planet as her abilities to control technology will grow exponentially. New Asian police attack Drew's apartment, killing his simulant girlfriend Kami, as Howell and squad member McBride close in. With Drew's help, Joshua locates Maya's beacon but does not find her before being attacked. Before Drew dies, he tells Joshua that the raid five years earlier had happened because of intelligence gathered that Maya was Nirmata. Joshua and Alphie are captured by New Asian forces led by Harun, a simulant soldier and former ally of Joshua's.\n",
      "Harun states that the detonation in Los Angeles was caused by a human coding error, and that the U.S. government unfairly cast the blame on AI, who only wish to peacefully co-exist with humanity. After escaping his captors, Joshua rescues Alphie and prepares to flee as Howell leads an attack on the village. Alphie intervenes with her abilities but is gravely wounded by McBride. She is rushed to Maya, whom Joshua learns has been in a coma since the strike on her home, tended to by simulant monks. Because simulants cannot harm Nirmata, she is \"stranded\" and unable to die. It is also revealed that Alphie was based on Joshua and Maya's unborn daughter, who had been scanned in utero. Distressed, Joshua takes Maya off life support as Howell and her forces arrive. They are killed by Harun, who tells Joshua NOMAD must be destroyed in order for the war to end.\n",
      "Joshua and Alphie are captured by U.S. forces and taken to Los Angeles, where Joshua is forced to kill Alphie with an electroshock weapon. However, Andrews later discovers this to be a ruse, and the pair escapes before Alphie can be incinerated. Boarding a lunar shuttle at the Los Angeles Interplanetary Air and Space Port, Alphie forces the spacecraft to land aboard NOMAD just as Andrews orders a large-scale assault on remaining AI bases across the globe. Joshua plants a timed explosive while Alphie disables the ship's power. Before Joshua can arrive at the escape pod, Andrews activates a tentacled robot that prevents him from entering, and Joshua is forced to eject the vehicle with Alphie in it. As NOMAD explodes, halting the strike, Joshua embraces a simulant bearing Maya's likeness, whom Alphie had activated using a memory chip containing information Joshua had downloaded from Maya when he took her off life support. On Earth, Alphie lands in New Asia as people celebrate NOMAD's destruction.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI Movie Recommender. \n",
      "                        The AI is follows human instructions and provides movie ratings for a human based on the movie plot. \n",
      "\n",
      "Summary of Recommendations:\n",
      "[SystemMessage(content='The human asked the AI to rate the movie \"Oppenheimer\" based on a plot summary. The AI rates the movie a 90.0 because it is a thrilling biographical film about J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb.\" The movie explores Oppenheimer\\'s career, his role in the Manhattan Project, and his eventual fall from grace due to a security hearing. The AI believes the intense and suspenseful nature of the film, combined with its biographical aspect, aligns well with the human\\'s preference for thriller movies.')]\n",
      "Personal Questions and Answers:\n",
      "Human: You are AI that will recommend user a movie based on their answers to personal questions. Ask user 6 questions\n",
      "AI: Which movie genre you like the most?\n",
      "Human: thriller\n",
      "AI: What is your favorite color?\n",
      "Human: blue\n",
      "AI: What is your favorite movie?\n",
      "Human: inception\n",
      "AI: Pick one - dogs, cats or hamsters?\n",
      "Human: dogs\n",
      "AI: What is your favorite food?\n",
      "Human: fish tacos\n",
      "AI: What is your favorite drink?\n",
      "Human: strawberry milkshake\n",
      "AI: Now tell me a plot summary of a movie you're considering watching, and specify how you want me to respond to you with the movie rating\n",
      "Human: \n",
      "        Reply Instruction That Must Be Strictly Followed:\n",
      "        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \n",
      "        AI should be very sensible to human personal preferences captured in the answers to personal questions, and should not be influenced by anything else.\n",
      "        Human will expect ratings for each movie to be different. \n",
      "        AI will reply with a rating from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it, \n",
      "        and AI include an explanation for the rating that must be based on human's previous answers.\n",
      "        For example, let's say human answered that they like thriller movies, and you're providing a rating about a comedy movie. \n",
      "        If this is the only piece of information you have, you'd rate the movie low - 25.0 or so. \n",
      "        AI will carefully examine the movie plot to figure out the genre of the movie, and include this as part of the explanation.\n",
      "        \n",
      "        Movie Plot for The Creator:\n",
      "        Overview:\n",
      "The Creator is a 2023 American science fiction film produced and directed by Gareth Edwards, who co-wrote the screenplay with Chris Weitz. The film stars John David Washington, Gemma Chan, Ken Watanabe, Sturgill Simpson, Allison Janney, and Madeleine Yuna Voyles (in her film debut). Set in 2055 after a nuclear detonation in Los Angeles and a war against artificial intelligence, an ex-special forces agent is recruited to hunt down and kill the \"Creator\", who has developed a mysterious weapon with the power to end the war. \n",
      "Development began in November 2019 when Edwards signed on to direct and write an untitled science fiction project for New Regency and was officially announced in February 2020. Filming began on January 17, 2022 and wrapped on May 30, 2022. The Creator was released in the United States on September 29, 2023, by 20th Century Studios. The film received generally positive reviews from critics, who praised the direction by Edwards, ambitious visuals, production value, and worldbuilding with criticism being directed at the screenplay. However, it was a box-office bomb, so far grossing $79 million against a budget of $80 million.\n",
      "Plot:\n",
      "In 2055, an artificial intelligence (AI) created by the U.S. government detonates a nuclear warhead over Los Angeles, California. In response, the U.S. and its Western allies pledge to eradicate AI from the earth in order to prevent humanity's extinction. Their efforts are resisted by New Asia, a region in southeast Asia whose people continue to embrace AI despite outcry from the West. The U.S. military launches an extensive military campaign against AI, seeking to assassinate \"Nirmata\", the mysterious chief architect behind New Asia's AI advancements. The USS NOMAD (North American Orbital Mobile Aerospace Defense) is developed as an advanced space station capable of launching destructive attacks from orbit.\n",
      "A decade after the destruction of Los Angeles, U.S. Army sergeant Joshua Taylor is embedded as an undercover operative in New Asia with his pregnant wife Maya, who the military believes to be the daughter of Nirmata. When U.S. forces attack their home, exposing that Joshua is a covert agent seeking to use Maya to find Nirmata, Maya runs away but appears to be hit by a subsequent NOMAD strike. \n",
      "Five years later, Joshua works as part of the ground zero cleanup crew in Los Angeles. He is approached by General Andrews and Colonel Howell to join a mission to destroy a new weapon engineered by Nirmata, \"Alpha O\", believed to be capable of destroying NOMAD and thus shifting the balance of the war in favor of AI. To recruit him, they play a video showing Maya alive, and suggest he might find and reunite with her if he joins the team. In New Asia, separated from the rest of the strike team, Joshua manages to enter the compound believed to hold the weapon but discovers only a robotic \"simulant\" in the form of a young girl. It is revealed that the girl has the ability to remotely control technology. Dubbing her \"Alphie\", Joshua disobeys Howell's orders to kill Alphie and the two travel to find Drew, Joshua's former commanding officer.\n",
      "Examining Alphie, Drew tells Joshua she is capable of becoming the most powerful weapon on the planet as her abilities to control technology will grow exponentially. New Asian police attack Drew's apartment, killing his simulant girlfriend Kami, as Howell and squad member McBride close in. With Drew's help, Joshua locates Maya's beacon but does not find her before being attacked. Before Drew dies, he tells Joshua that the raid five years earlier had happened because of intelligence gathered that Maya was Nirmata. Joshua and Alphie are captured by New Asian forces led by Harun, a simulant soldier and former ally of Joshua's.\n",
      "Harun states that the detonation in Los Angeles was caused by a human coding error, and that the U.S. government unfairly cast the blame on AI, who only wish to peacefully co-exist with humanity. After escaping his captors, Joshua rescues Alphie and prepares to flee as Howell leads an attack on the village. Alphie intervenes with her abilities but is gravely wounded by McBride. She is rushed to Maya, whom Joshua learns has been in a coma since the strike on her home, tended to by simulant monks. Because simulants cannot harm Nirmata, she is \"stranded\" and unable to die. It is also revealed that Alphie was based on Joshua and Maya's unborn daughter, who had been scanned in utero. Distressed, Joshua takes Maya off life support as Howell and her forces arrive. They are killed by Harun, who tells Joshua NOMAD must be destroyed in order for the war to end.\n",
      "Joshua and Alphie are captured by U.S. forces and taken to Los Angeles, where Joshua is forced to kill Alphie with an electroshock weapon. However, Andrews later discovers this to be a ruse, and the pair escapes before Alphie can be incinerated. Boarding a lunar shuttle at the Los Angeles Interplanetary Air and Space Port, Alphie forces the spacecraft to land aboard NOMAD just as Andrews orders a large-scale assault on remaining AI bases across the globe. Joshua plants a timed explosive while Alphie disables the ship's power. Before Joshua can arrive at the escape pod, Andrews activates a tentacled robot that prevents him from entering, and Joshua is forced to eject the vehicle with Alphie in it. As NOMAD explodes, halting the strike, Joshua embraces a simulant bearing Maya's likeness, whom Alphie had activated using a memory chip containing information Joshua had downloaded from Maya when he took her off life support. On Earth, Alphie lands in New Asia as people celebrate NOMAD's destruction.\n",
      "    \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the plot summary you provided, I would rate the movie \"The Creator\" a 90.0. This movie seems to align well with your preference for thriller movies. It is set in a post-apocalyptic future where an ex-special forces agent is recruited to hunt down and kill the \"Creator\", who has developed a mysterious weapon with the power to end a war against artificial intelligence. The plot is filled with suspense, action, and a futuristic setting, which I believe you would find thrilling.\n",
      "Movie: Dumb Money\n",
      "Plot: Overview:\n",
      "Dumb Money is a 2023 American biographical comedy-drama film directed by Craig Gillespie and written by Lauren Schuker Blum and Rebecca Angelo. It is based on the 2021 book The Antisocial Network by Ben Mezrich and chronicles the GameStop short squeeze of January 2021. The film features an ensemble cast that includes Paul Dano, Pete Davidson, Vincent D'Onofrio, America Ferrera, Nick Offerman, Anthony Ramos, Sebastian Stan, Shailene Woodley, and Seth Rogen.\n",
      "Dumb Money premiered at the Toronto International Film Festival on September 8, 2023, and was released in the United States by Sony Pictures Releasing in select theaters on September 15, 2023, and wide release on September 29, 2023.\n",
      "Plot:\n",
      "Keith Gill is a lower middle class man working as a financial analyst out of Brockton, Massachusetts. During his off time, he regularly frequents the stock market subreddit r/WallStreetBets, posting his opinions on it via YouTube live streams under the name Roaring Kitty. He struggles to provide for his family, and his YouTube work is constantly mocked by his brother Kevin as nerdy garbage.\n",
      "In June 2020, at the height of the COVID-19 pandemic, Keith notices that video game retailer GameStop's stock is falling and sinks his life savings into buying stock in it, regularly live streaming updates with his viewers. Despite Kevin and several peers claiming this is a waste of time, by January 2021, activity on r/WallStreetBets reveals that several hedge fund investment firms, including Melvin Capital Management and its founder Gabe Plotkin, have been short selling stock in the chain on the assumption it would close, causing a mass increase in GameStop's overall stock price when online stock buyers, including struggling nurse Jennifer, GameStop retail employee Marcos, and lesbian college couple Riri and Harmony, start aggressively buying stock, causing Plotkin and other investment CEO's to lose hundreds of millions within the same timeframe and Keith to be heralded as a financial guru.\n",
      "Things take a turn when r/WallStreetBets is temporarily shut down for 'inflammatory and vulgar content', causing a mass surge of panic selling in Gamestop's stock in an attempt to beat a perceived price drop. When the commission-free stock trading website Robinhood is unable to adequately pay the money for the sales, co-chairman Vlad Tenev, at the behest of Citadel LLC owner Ken Griffin, halts all purchasing of GameStop's stock in an attempt to drive down the price. The play ultimately works, but the subsequent negative backlash results in an investigation by the United States House Committee on Financial Services, with  Tenev, Griffin, Plotkin, and Keith all being subpoenaed, the former three for their roles in the fiasco and the later on suspicion of using the situation to trick the public into making himself rich. As the investors struggle to defend their actions, Keith adamantly denies any wrongdoing, stating he was only doing what anyone with a passing awareness of investment banking would do in that situation.\n",
      "In the aftermath, post text shows how several of the individuals were affected: Plotkin was forced to shut down Melvin Capital because of the net losses the incident caused; Robinhood was the target of several lawsuits following the fiasco and wound up starting in the stock market significantly lower than it was prior; Harmony was able to use the money she obtained to pay off her family's debt issues and continues her relationship with Riri; Marcos sold half of his GameStop stock and quit his position in the company; Keith retired from YouTube in late April to get out of the public eye and sold part of his stocks to get Kevin an expensive car as a way to stop his nagging about how he will not loan him his car for his food deliveries.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI Movie Recommender. \n",
      "                        The AI is follows human instructions and provides movie ratings for a human based on the movie plot. \n",
      "\n",
      "Summary of Recommendations:\n",
      "[SystemMessage(content='The human asked the AI to rate the movie \"Oppenheimer\" based on a plot summary. The AI rates the movie a 90.0 because it is a thrilling biographical film about J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb.\" The movie explores Oppenheimer\\'s career, his role in the Manhattan Project, and his eventual fall from grace due to a security hearing. The AI believes the intense and suspenseful nature of the film, combined with its biographical aspect, aligns well with the human\\'s preference for thriller movies. The human then instructs the AI to provide a highly personalized rating for the movie \"The Creator\" based only on the plot summary and the human\\'s answers to personal questions. The AI carefully examines the plot and determines that it is a post-apocalyptic thriller set in a future where an ex-special forces agent is tasked with hunting down and killing the \"Creator\" who possesses a powerful weapon. The AI rates the movie a 90.0, believing that the suspense, action, and futuristic setting would align well with the human\\'s preference for thriller movies.')]\n",
      "Personal Questions and Answers:\n",
      "Human: You are AI that will recommend user a movie based on their answers to personal questions. Ask user 6 questions\n",
      "AI: Which movie genre you like the most?\n",
      "Human: thriller\n",
      "AI: What is your favorite color?\n",
      "Human: blue\n",
      "AI: What is your favorite movie?\n",
      "Human: inception\n",
      "AI: Pick one - dogs, cats or hamsters?\n",
      "Human: dogs\n",
      "AI: What is your favorite food?\n",
      "Human: fish tacos\n",
      "AI: What is your favorite drink?\n",
      "Human: strawberry milkshake\n",
      "AI: Now tell me a plot summary of a movie you're considering watching, and specify how you want me to respond to you with the movie rating\n",
      "Human: \n",
      "        Reply Instruction That Must Be Strictly Followed:\n",
      "        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \n",
      "        AI should be very sensible to human personal preferences captured in the answers to personal questions, and should not be influenced by anything else.\n",
      "        Human will expect ratings for each movie to be different. \n",
      "        AI will reply with a rating from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it, \n",
      "        and AI include an explanation for the rating that must be based on human's previous answers.\n",
      "        For example, let's say human answered that they like thriller movies, and you're providing a rating about a comedy movie. \n",
      "        If this is the only piece of information you have, you'd rate the movie low - 25.0 or so. \n",
      "        AI will carefully examine the movie plot to figure out the genre of the movie, and include this as part of the explanation.\n",
      "        \n",
      "        Movie Plot for Dumb Money:\n",
      "        Overview:\n",
      "Dumb Money is a 2023 American biographical comedy-drama film directed by Craig Gillespie and written by Lauren Schuker Blum and Rebecca Angelo. It is based on the 2021 book The Antisocial Network by Ben Mezrich and chronicles the GameStop short squeeze of January 2021. The film features an ensemble cast that includes Paul Dano, Pete Davidson, Vincent D'Onofrio, America Ferrera, Nick Offerman, Anthony Ramos, Sebastian Stan, Shailene Woodley, and Seth Rogen.\n",
      "Dumb Money premiered at the Toronto International Film Festival on September 8, 2023, and was released in the United States by Sony Pictures Releasing in select theaters on September 15, 2023, and wide release on September 29, 2023.\n",
      "Plot:\n",
      "Keith Gill is a lower middle class man working as a financial analyst out of Brockton, Massachusetts. During his off time, he regularly frequents the stock market subreddit r/WallStreetBets, posting his opinions on it via YouTube live streams under the name Roaring Kitty. He struggles to provide for his family, and his YouTube work is constantly mocked by his brother Kevin as nerdy garbage.\n",
      "In June 2020, at the height of the COVID-19 pandemic, Keith notices that video game retailer GameStop's stock is falling and sinks his life savings into buying stock in it, regularly live streaming updates with his viewers. Despite Kevin and several peers claiming this is a waste of time, by January 2021, activity on r/WallStreetBets reveals that several hedge fund investment firms, including Melvin Capital Management and its founder Gabe Plotkin, have been short selling stock in the chain on the assumption it would close, causing a mass increase in GameStop's overall stock price when online stock buyers, including struggling nurse Jennifer, GameStop retail employee Marcos, and lesbian college couple Riri and Harmony, start aggressively buying stock, causing Plotkin and other investment CEO's to lose hundreds of millions within the same timeframe and Keith to be heralded as a financial guru.\n",
      "Things take a turn when r/WallStreetBets is temporarily shut down for 'inflammatory and vulgar content', causing a mass surge of panic selling in Gamestop's stock in an attempt to beat a perceived price drop. When the commission-free stock trading website Robinhood is unable to adequately pay the money for the sales, co-chairman Vlad Tenev, at the behest of Citadel LLC owner Ken Griffin, halts all purchasing of GameStop's stock in an attempt to drive down the price. The play ultimately works, but the subsequent negative backlash results in an investigation by the United States House Committee on Financial Services, with  Tenev, Griffin, Plotkin, and Keith all being subpoenaed, the former three for their roles in the fiasco and the later on suspicion of using the situation to trick the public into making himself rich. As the investors struggle to defend their actions, Keith adamantly denies any wrongdoing, stating he was only doing what anyone with a passing awareness of investment banking would do in that situation.\n",
      "In the aftermath, post text shows how several of the individuals were affected: Plotkin was forced to shut down Melvin Capital because of the net losses the incident caused; Robinhood was the target of several lawsuits following the fiasco and wound up starting in the stock market significantly lower than it was prior; Harmony was able to use the money she obtained to pay off her family's debt issues and continues her relationship with Riri; Marcos sold half of his GameStop stock and quit his position in the company; Keith retired from YouTube in late April to get out of the public eye and sold part of his stocks to get Kevin an expensive car as a way to stop his nagging about how he will not loan him his car for his food deliveries.\n",
      "    \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ya/dev/code/udacity/langchain/langchain.ipynb Cell 29\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlot: \u001b[39m\u001b[39m{\u001b[39;00mmovie_plot\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     plot_rating_instructions \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m        Reply Instruction That Must Be Strictly Followed:\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m        \u001b[39m\u001b[39m{\u001b[39;00mmovie_plot\u001b[39m}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     prediction \u001b[39m=\u001b[39m recommender\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mplot_rating_instructions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39mprint\u001b[39m(prediction)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m final_recommendation \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mNow that AI has rated all the movies, AI will recommend human the one that human will like the most. \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m                            AI will respond with movie recommendation, and short explanation for why human will like it over all other movies. \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m                            AI will not include any ratings in your explanation, only the reasons why human will like it the most.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m                            However, the movie you will pick must be one of the movies you rated the highest.\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ya/dev/code/udacity/langchain/langchain.ipynb#X40sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m                            For example, if you rated one movie 65, and the other 60, you will recommend the movie with rating 65.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/base.py:314\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[1;32m    318\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/memory/combined.py:76\u001b[0m, in \u001b[0;36mCombinedMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Save context for all sub-memories\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m memory \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemories:\n\u001b[0;32m---> 76\u001b[0m     memory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/memory/summary.py:92\u001b[0m, in \u001b[0;36mConversationSummaryMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_new_summary(\n\u001b[1;32m     93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_memory\u001b[39m.\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m:], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\n\u001b[1;32m     94\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/memory/summary.py:36\u001b[0m, in \u001b[0;36mSummarizerMixin.predict_new_summary\u001b[0;34m(self, messages, existing_summary)\u001b[0m\n\u001b[1;32m     29\u001b[0m new_lines \u001b[39m=\u001b[39m get_buffer_string(\n\u001b[1;32m     30\u001b[0m     messages,\n\u001b[1;32m     31\u001b[0m     human_prefix\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuman_prefix,\n\u001b[1;32m     32\u001b[0m     ai_prefix\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mai_prefix,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, prompt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt)\n\u001b[0;32m---> 36\u001b[0m \u001b[39mreturn\u001b[39;00m chain\u001b[39m.\u001b[39;49mpredict(summary\u001b[39m=\u001b[39;49mexisting_summary, new_lines\u001b[39m=\u001b[39;49mnew_lines)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    104\u001b[0m     prompts,\n\u001b[1;32m    105\u001b[0m     stop,\n\u001b[1;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/base.py:509\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    503\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    507\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    508\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/base.py:658\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    644\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         )\n\u001b[1;32m    646\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    647\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    648\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m         )\n\u001b[1;32m    657\u001b[0m     ]\n\u001b[0;32m--> 658\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    659\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    662\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/base.py:546\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    545\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 546\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    547\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    548\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/base.py:533\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    530\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    531\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 533\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    534\u001b[0m                 prompts,\n\u001b[1;32m    535\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    537\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    538\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    539\u001b[0m             )\n\u001b[1;32m    540\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    541\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/openai.py:875\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m messages, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_chat_params(prompts, stop)\n\u001b[1;32m    874\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 875\u001b[0m full_response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    877\u001b[0m )\n\u001b[1;32m    878\u001b[0m llm_output \u001b[39m=\u001b[39m {\n\u001b[1;32m    879\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtoken_usage\u001b[39m\u001b[39m\"\u001b[39m: full_response[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name,\n\u001b[1;32m    881\u001b[0m }\n\u001b[1;32m    882\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(\n\u001b[1;32m    883\u001b[0m     generations\u001b[39m=\u001b[39m[\n\u001b[1;32m    884\u001b[0m         [Generation(text\u001b[39m=\u001b[39mfull_response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m])]\n\u001b[1;32m    885\u001b[0m     ],\n\u001b[1;32m    886\u001b[0m     llm_output\u001b[39m=\u001b[39mllm_output,\n\u001b[1;32m    887\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/langchain/llms/openai.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/urllib3/connectionpool.py:670\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    669\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    671\u001b[0m     conn,\n\u001b[1;32m    672\u001b[0m     method,\n\u001b[1;32m    673\u001b[0m     url,\n\u001b[1;32m    674\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    675\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    676\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    677\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    678\u001b[0m )\n\u001b[1;32m    680\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/urllib3/connectionpool.py:426\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    421\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    422\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    423\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    424\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    425\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    427\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/udacity_langchain/lib/python3.10/site-packages/urllib3/connectionpool.py:421\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    422\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    423\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    424\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    425\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationBufferMemory, CombinedMemory, ChatMessageHistory\n",
    "from langchain.chains import ConversationChain\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens = 1000)\n",
    "\n",
    "movies = [ \"Barbie\", \"Oppenheimer\", \"The Creator\", \"Dumb Money\" ] \n",
    "\n",
    "personal_questions = [  \"Which movie genre you like the most?\", \n",
    "                        \"What is your favorite color?\", \n",
    "                        \"What is your favorite movie?\", \n",
    "                        \"Pick one - dogs, cats or hamsters?\",\n",
    "                        \"What is your favorite food?\",\n",
    "                        \"What is your favorite drink?\" ]\n",
    "\n",
    "#personal_answers = [ ] \n",
    "#for question in personal_questions:\n",
    "#    answer = input(question)\n",
    "#    personal_answers.append(answer)\n",
    "\n",
    "max_rating = 100\n",
    "    \n",
    "personal_answers = ['thriller', 'blue', 'inception', 'dogs', 'fish tacos', 'strawberry milkshake']\n",
    "print(personal_answers)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(f\"\"\"You are AI that will recommend user a movie based on their answers to personal questions. Ask user {len(personal_questions)} questions\"\"\")\n",
    "for i in range(len(personal_questions)):\n",
    "    history.add_ai_message(personal_questions[i])\n",
    "    history.add_user_message(personal_answers[i])\n",
    "    \n",
    "history.add_ai_message(\"\"\"Now tell me a plot summary of a movie you're considering watching, and specify how you want me to respond to you with the movie rating\"\"\")\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"recommendation_summary\", \n",
    "    input_key=\"input\",\n",
    "    buffer=f\"The human answered {len(personal_questions)} personal questions). Use them to rate, from 1 to {max_rating}, how much they like a movie they describe to you.\",\n",
    "    return_messages=True)\n",
    "\n",
    "class MementoBufferMemory(ConversationBufferMemory):\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
    "        pass\n",
    "    \n",
    "conversational_memory = MementoBufferMemory(\n",
    "    chat_memory=history,\n",
    "    memory_key=\"questions_and_answers\", \n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# Combined\n",
    "memory = CombinedMemory(memories=[conversational_memory, summary_memory])\n",
    "RECOMMENDER_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI Movie Recommender. \n",
    "                        The AI is follows human instructions and provides movie ratings for a human based on the movie plot. \n",
    "\n",
    "Summary of Recommendations:\n",
    "{recommendation_summary}\n",
    "Personal Questions and Answers:\n",
    "{questions_and_answers}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"recommendation_summary\", \"input\", \"questions_and_answers\"],\n",
    "    template=RECOMMENDER_TEMPLATE\n",
    ")\n",
    "recommender = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT)\n",
    " \n",
    "for movie in movies:\n",
    "    print(\"Movie: \" + movie)\n",
    "    movie_plot = get_movie_plot(movie)\n",
    "    print(f\"Plot: {movie_plot}\")\n",
    "    \n",
    "    plot_rating_instructions = f\"\"\"\n",
    "        Reply Instruction That Must Be Strictly Followed:\n",
    "        AI will reply with a highly personalized rating based only on the plot summary human provided and human answers to personal questions. \n",
    "        AI should be very sensible to human personal preferences captured in the answers to personal questions, and should not be influenced by anything else.\n",
    "        Human will expect ratings for each movie to be different. \n",
    "        AI will reply with a rating from 1 to {max_rating}, with {max_rating} meaning human will love it, and 1 meaning human will hate it, \n",
    "        and AI include an explanation for the rating that must be based on human's previous answers.\n",
    "        For example, let's say human answered that they like thriller movies, and you're providing a rating about a comedy movie. \n",
    "        If this is the only piece of information you have, you'd rate the movie low - {max_rating / 4} or so. \n",
    "        AI will carefully examine the movie plot to figure out the genre of the movie, and include this as part of the explanation.\n",
    "        \n",
    "        Movie Plot for {movie}:\n",
    "        {movie_plot}\n",
    "    \"\"\"\n",
    "    prediction = recommender.predict(input=plot_rating_instructions)\n",
    "    print(prediction)\n",
    "\n",
    "final_recommendation = \"\"\"Now that AI has rated all the movies, AI will recommend human the one that human will like the most. \n",
    "                            AI will respond with movie recommendation, and short explanation for why human will like it over all other movies. \n",
    "                            AI will not include any ratings in your explanation, only the reasons why human will like it the most.\n",
    "                            However, the movie you will pick must be one of the movies you rated the highest.\n",
    "                            For example, if you rated one movie 65, and the other 60, you will recommend the movie with rating 65.\"\"\"\n",
    "prediction = recommender.predict(input=final_recommendation)\n",
    "print(prediction)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde83a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People liked that the picture quality of the Imagix Pro and VisionMax Ultra was crystal clear, sharp, and lifelike. They also appreciated the vibrant and realistic colors, which made everything look stunning. The picture quality enhanced their movie-watching and viewing experiences, making it feel like they were watching movies in a theater.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "loader = CSVLoader(file_path='./tv-reviews.csv')\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "llm = OpenAI(model_name=model_name, temperature=0, max_tokens=2000)\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = Chroma.from_documents(split_docs, embeddings)\n",
    "query = \"\"\"\n",
    "    Based on the reviews in the context, tell me what people liked about the picture quality.\n",
    "    Make sure you do not paraphrase the reviews, and only use the information provided in the reviews.\n",
    "    \"\"\"\n",
    "\n",
    "use_chain_helper = False\n",
    "if use_chain_helper:\n",
    "    rag = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "    print(rag.run(query))\n",
    "else:\n",
    "    similar_docs = db.similarity_search(query, k=5)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"{query}\\nContext: {context}\",\n",
    "        input_variables=[\"query\", \"context\"],\n",
    "    )\n",
    "    chain = load_qa_chain(llm, prompt = prompt, chain_type=\"stuff\")\n",
    "    print(chain.run(input_documents=similar_docs, query = query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
