{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9d6626",
   "metadata": {},
   "source": [
    "# Steps 3 & 4: Querying a Completion Model with a Custom Text Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0478d18",
   "metadata": {},
   "source": [
    "Add your API key to the cell below then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa50906",
   "metadata": {},
   "source": [
    "The code below loads in the data sorted by cosine distance that you previously created. Run it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"distances.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757505cb",
   "metadata": {},
   "source": [
    "## TODO 1: Build the Custom Text Prompt\n",
    "\n",
    "Run the cell below as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# Create a tokenizer that is designed to align with our embeddings\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_limit = 1000\n",
    "USER_QUESTION = \"\"\"What were the estimated damages of the 2023 \\\n",
    "Turkey-Syria earthquake?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c0ca1",
   "metadata": {},
   "source": [
    "Now your task is to compose the custom text prompt.\n",
    "\n",
    "The overall structure of the prompt should look like this:\n",
    "\n",
    "```\n",
    "Answer the question based on the context below, and if the\n",
    "question can't be answered based on the context, say \"I don't\n",
    "know\"\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "```\n",
    "\n",
    "In the place marked `context`, provide as much information from `df['text']` as possible without exceeding `token_limit`. In the place marked `question`, add `USER_QUESTION`.\n",
    "\n",
    "Your overall goal is to create a string called `prompt` that contains all of the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de013ac",
   "metadata": {},
   "source": [
    "If you're getting stuck, you can click to reveal the solution then copy and paste this into the cell below.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><strong>Solution (click to show/hide)</strong></summary>\n",
    "\n",
    "```python\n",
    "# Count the number of tokens in the prompt template and question\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the \n",
    "question can't be answered based on the context, say \n",
    "\"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                        len(tokenizer.encode(USER_QUESTION))\n",
    "\n",
    "# Create a list to store text for context\n",
    "context_list = []\n",
    "\n",
    "# Loop over rows of the sorted dataframe\n",
    "for text in df[\"text\"].values:\n",
    "    \n",
    "    # Append text to context_list if there is enough room\n",
    "    token_count += len(tokenizer.encode(text))\n",
    "    if token_count <= token_limit:\n",
    "        context_list.append(text)\n",
    "    else:\n",
    "        # Break once we're over the token limit\n",
    "        break\n",
    "\n",
    "# Use string formatting to complete the prompt\n",
    "prompt = prompt_template.format(\n",
    "    \"\\n\\n###\\n\\n\".join(context_list),\n",
    "    USER_QUESTION\n",
    ")\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tokens in the prompt template and question\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the \n",
    "question can't be answered based on the context, say \n",
    "\"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "token_count = \n",
    "\n",
    "# Create a list to store text for context\n",
    "context_list = \n",
    "\n",
    "# Loop over rows of the sorted dataframe\n",
    "for text in df[\"text\"].values:\n",
    "    \n",
    "    # Append text to context_list if there is enough room\n",
    "    \n",
    "\n",
    "# Use string formatting to complete the prompt\n",
    "prompt = prompt_template.format(\n",
    "    \"\\n\\n###\\n\\n\".join(context_list),\n",
    "    USER_QUESTION\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097f24d",
   "metadata": {},
   "source": [
    "## TODO 2: Send Custom Text Prompt to Completion Model\n",
    "\n",
    "Using the `prompt` string you created, query an OpenAI `Completion` model to get an answer. Specify a `max_tokens` of 150.\n",
    "\n",
    "If you're getting stuck, you can click to reveal the solution then copy and paste this into the cell below.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><strong>Solution (click to show/hide)</strong></summary>\n",
    "\n",
    "```python\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "response = openai.Completion.create(\n",
    "    model=COMPLETION_MODEL_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=150\n",
    ")\n",
    "answer = response[\"choices\"][0][\"text\"].strip()\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b35a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "response = \n",
    "answer = \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70191209",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations ðŸŽ‰\n",
    "\n",
    "You have now completed the prompt engineering process using unsupervised ML to get a custom answer from an OpenAI model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
