{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "### Explaination of Why selecting this data\n",
    "\n",
    "Chat GPT took the world by storm to demonstrate how LLMs can do amazing work. Open AI (the company behind Chat GPT) has amazing history. Furthermore, as Chat GPT became more famous and adapted, Open AI started getting limelight for both good and bad reasons. So its natural for someone to follow what has happened at Open AI and what are they recently working into. \n",
    "\n",
    "However, the current LLM model will not provide us with this details. Hence, I am using the wiki which gets updated in real-time to updates about Open AI. Given that the wiki page will keep updated as more open AI news comes out, I wanted to use this page so that we can keep following Open AI along its journey\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f9a2c",
   "metadata": {},
   "source": [
    "## Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d542ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "openai.api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "TODO: In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f786d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str):\n",
    "    headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # Todo: fetch the page using the GET requests\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        return r.text\n",
    "    else:\n",
    "        return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Wikipedia page for list of Sports events in 2024\n",
    "params = {\n",
    "    \"action\": \"query\", \n",
    "    \"prop\": \"extracts\",\n",
    "    \"exlimit\": 1,\n",
    "    \"titles\": \"OpenAI\",\n",
    "    \"explaintext\": 1,\n",
    "    \"formatversion\": 2,\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "resp = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params)\n",
    "response_dict = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a595980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 rows in the dataset\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>– OpenAI is an American artificial intelligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>– The organization consists of the non-profit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>– In November 2023, OpenAI's board removed Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>– In December 2015, OpenAI was founded by Sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>– According to Wired, Brockman met with Yoshu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   – OpenAI is an American artificial intelligen...\n",
       "1   – The organization consists of the non-profit...\n",
       "2   – In November 2023, OpenAI's board removed Sa...\n",
       "3   – In December 2015, OpenAI was founded by Sam...\n",
       "4   – According to Wired, Brockman met with Yoshu..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = response_dict['query']['pages'][0]['extract'].split(\"\\n\")\n",
    "\n",
    "# Load page text into a dataframe\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = text_data\n",
    "\n",
    "# Clean up dataframe to remove empty lines and headings\n",
    "df = df[(\n",
    "    (df[\"text\"].str.len() > 0) & (~df[\"text\"].str.startswith(\"==\"))\n",
    ")].reset_index(drop=True)\n",
    "\n",
    "# In some cases dates are used as headings instead of being part of the\n",
    "# text sample; adjust so dated text samples start with dates\n",
    "prefix = \"\"\n",
    "for (i, row) in df.iterrows():\n",
    "    # If the row already has \" - \", it already has the needed date prefix\n",
    "    if \" – \" not in row[\"text\"]:\n",
    "        try:\n",
    "            # If the row's text is a date, set it as the new prefix\n",
    "            parse(row[\"text\"])\n",
    "            prefix = row[\"text\"]\n",
    "        except:\n",
    "            # If the row's text isn't a date, add the prefix\n",
    "            row[\"text\"] = prefix + \" – \" + row[\"text\"]\n",
    "df = df[df[\"text\"].str.contains(\" – \")]\n",
    "df\n",
    "\n",
    "print('There are ' + str(len(df)) + ' rows in the dataset') \n",
    "print('')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n",
    "\n",
    "TODO: In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model. You may copy and paste any useful code from the course materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade3328",
   "metadata": {},
   "source": [
    "### Generating Embeddings\n",
    "\n",
    "We'll use the `Embedding` tooling from OpenAI documentation here to create vectors representing each row of our custom dataset.\n",
    "\n",
    "In order to avoid a RateLimitError we'll send our data in batches to the `Embedding.create` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>– OpenAI is an American artificial intelligen...</td>\n",
       "      <td>[-0.019855355843901634, -0.023019928485155106,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>– The organization consists of the non-profit...</td>\n",
       "      <td>[-0.006303194910287857, -0.03769782558083534, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>– In November 2023, OpenAI's board removed Sa...</td>\n",
       "      <td>[-0.008517647162079811, -0.04329528659582138, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>– In December 2015, OpenAI was founded by Sam...</td>\n",
       "      <td>[0.00842413678765297, -0.029996534809470177, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>– According to Wired, Brockman met with Yoshu...</td>\n",
       "      <td>[-0.011349588632583618, -0.01666202023625374, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>– OpenAI on X</td>\n",
       "      <td>[0.018376905471086502, -0.02012915536761284, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>– OpenAI on Instagram</td>\n",
       "      <td>[-0.024862777441740036, -0.007472761906683445,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>– OpenAI on YouTube</td>\n",
       "      <td>[-0.016021838411688805, -0.023299461230635643,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>– \"What OpenAI Really Wants\" by Wired</td>\n",
       "      <td>[-0.00038224150193855166, -0.00625555776059627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>– \"The Inside Story of Microsoft's Partnershi...</td>\n",
       "      <td>[0.008858347311615944, -0.029148975387215614, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0     – OpenAI is an American artificial intelligen...   \n",
       "1     – The organization consists of the non-profit...   \n",
       "2     – In November 2023, OpenAI's board removed Sa...   \n",
       "3     – In December 2015, OpenAI was founded by Sam...   \n",
       "4     – According to Wired, Brockman met with Yoshu...   \n",
       "..                                                 ...   \n",
       "141                                      – OpenAI on X   \n",
       "142                              – OpenAI on Instagram   \n",
       "143                                – OpenAI on YouTube   \n",
       "144              – \"What OpenAI Really Wants\" by Wired   \n",
       "145   – \"The Inside Story of Microsoft's Partnershi...   \n",
       "\n",
       "                                            embeddings  \n",
       "0    [-0.019855355843901634, -0.023019928485155106,...  \n",
       "1    [-0.006303194910287857, -0.03769782558083534, ...  \n",
       "2    [-0.008517647162079811, -0.04329528659582138, ...  \n",
       "3    [0.00842413678765297, -0.029996534809470177, -...  \n",
       "4    [-0.011349588632583618, -0.01666202023625374, ...  \n",
       "..                                                 ...  \n",
       "141  [0.018376905471086502, -0.02012915536761284, -...  \n",
       "142  [-0.024862777441740036, -0.007472761906683445,...  \n",
       "143  [-0.016021838411688805, -0.023299461230635643,...  \n",
       "144  [-0.00038224150193855166, -0.00625555776059627...  \n",
       "145  [0.008858347311615944, -0.029148975387215614, ...  \n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Send text data to OpenAI model to get embeddings\n",
    "    response = openai.Embedding.create(\n",
    "        input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "# Add embeddings list to dataframe\n",
    "df[\"embeddings\"] = embeddings\n",
    "df.to_csv(\"embeddings.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d34f4",
   "metadata": {},
   "source": [
    "### Create Function to find all the text closet to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbd079",
   "metadata": {},
   "source": [
    "### Create a Function for text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4bb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "    \n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "        \n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "        \n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e007acd",
   "metadata": {},
   "source": [
    "### Create Function that answers questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e5b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4901c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "altman_departure_question = \"Why was Altman removed as the CEO?\"\n",
    "\n",
    "altman_departure_prompt = \"\"\"\n",
    "Question: {}\n",
    "Answer:\n",
    "\"\"\".format(altman_departure_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3d5c1",
   "metadata": {},
   "source": [
    "### Model's answer without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no specific reason mentioned for Altman's removal as CEO. It is possible that it was due to his performance or a decision made by the company's board of directors. Other factors such as conflict of interest, financial troubles, or lack of leadership skills may have also played a role. Without more information, it is impossible to determine the exact reason for his removal.\n"
     ]
    }
   ],
   "source": [
    "initial_altman_departure_answer = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=altman_departure_prompt,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(initial_altman_departure_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0ee18",
   "metadata": {},
   "source": [
    "### Model's answer with context from custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99764128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The board of directors cited a lack of confidence in him.\n"
     ]
    }
   ],
   "source": [
    "custom_altman_departure_answer = answer_question(altman_departure_question, df)\n",
    "print(custom_altman_departure_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f646989",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_intelligence_question = \"Which company did OpenAI partner at WWDC 2024 and for what reason?\"\n",
    "\n",
    "apple_intelligence_prompt = \"\"\"\n",
    "Question: {}\n",
    "Answer:\n",
    "\"\"\".format(apple_intelligence_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90b916",
   "metadata": {},
   "source": [
    "### Model's answer without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At WWDC 2024, OpenAI announced a partnership with Apple. The collaboration was focused on developing advanced artificial intelligence systems that could be integrated into Apple's products and services. This partnership aimed to enhance user experience and provide more personalized and intelligent features in Apple's devices and applications. The two companies also planned to work together on ethical and responsible AI practices, ensuring the technology is used for the benefit of society.\n"
     ]
    }
   ],
   "source": [
    "initial_apple_intelligence_answer = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=apple_intelligence_prompt,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(initial_apple_intelligence_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbc704",
   "metadata": {},
   "source": [
    "### Model's answer with context from custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1261b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc., to bring ChatGPT features to Apple Intelligence.\n"
     ]
    }
   ],
   "source": [
    "custom_apple_intelligence_answer = answer_question(apple_intelligence_question, df)\n",
    "print(custom_apple_intelligence_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec1c4a",
   "metadata": {},
   "source": [
    "### Enhanced perfomance post RAG\n",
    "\n",
    "As we can see from both questions, the turbo gpt 3.5 couldn't answer both questions correctly as it was not trained on the new specific avaialble data. For both questions, the model is hallucinating. \n",
    "\n",
    "1. For the first question about why Sam Altman was ousted?\n",
    "    a. For the initial (untrained answer), while the model has no idea why Sam Altman was removed, it is trying to reason that it could be have been due to his performance. \n",
    "    b. However, once we have provided the context, the model was able to find the correct answer that he was removed as the board lacked confidence in him.\n",
    "    \n",
    "2. For the second question, who Open AI partnered with at WWDC, and for what reason?\n",
    "    a. The model correctly predicted Apple.However,Apple is the biggest player for WWDC, so the model was able to correctly predict based on that probability. But it started hallucinating for reason why it partnered. The two companies never discuss about ethical use of AI and other reasons. \n",
    "    b. But, once provided the context was provided, it was able to corectly answer that the partnership was to bring ChatGPT to Apple intelligence.\n",
    "    \n",
    "So we can see clearly from above examples that the model performs appropriately after the context was provided with RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
